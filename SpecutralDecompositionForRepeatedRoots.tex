
\def\1{\y1}
\def\2{\y2}
\def\3{\y3}
\def\4{\y4}
\def\5{\y5}
\def\6{\y6}
\def\7{\y7}
\def\8{\y8}
\def\9{\y9}
\def\0{\y0}
\def\-{\y-}
\aaa{Spectural Decomposition}
Recall our setting

$$
F(x)=(x-Î»_1)^{n_1}â€¦(x-Î»_i)^{n_i}â€¦(x-Î»_k)^{n_k}
$$
We define $K_i(x)$ to be the \x{complementary factor} such that
$$
F(x)=K_i(x)(x-Î»_i)^{n_i}
$$
Note that $K_i(Î»_i)â‰ 0$.

The symbol $Îµ$ represents infinitesimal (very very small) and 
$$
âˆ := \frac1{Îµ}
$$
represents infinity.  We are allowing arithmetic calculation with symbols $âˆ$ and $Îµ$ so one can talk about $a_{-N}âˆ+\cdots+a_0+a_1Îµ+\cdots$.
\a\aa
Recall our original formula of partial fraction decomposition
$$
\frac{g(x)}{F(x)}=Q(x) + âˆ‘_{i=1}^kâ€œConstâ€œ_Îµ\left(g(Î»_i+Îµ)Â·\frac1{Îµ^{n_i-1}}Â·\frac1{K_i(Î»_i+Îµ)}Â·\frac1{x-Î»_i-Îµ}\right)
$$
We may multiply $F(x)$ on both sides and obtain
$$
g(x)=
$$
$$Q(x)F(x) + âˆ‘_{i=1}^k \underbrace{F(x)Â·â€œConstâ€œ_Îµ\left(g(Î»_i+Îµ)Â·{âˆ^{n_i-1}}Â·\frac1{K_i(Î»_i+Îµ)}Â·\frac1{x-Î»_i-Îµ}\right)}_{â€œinterpolation summandâ€œ}
$$
Why each interpolation summand a polynomial?
\a\aa

Since $$F(x)=â€¦+0âˆ^2+0âˆ+F(x)+0Îµ+0Îµ^2+â€¦$$, we may put $F(x)$ inside, and note $F(x)=(x-Î»_i)^{n_i}K_i(x)$ write
$$
â€œInterpolation summandâ€œ=â€œConstâ€œ_Îµ\left(g(Î»_i+Îµ)Â·{âˆ^{n_i-1}}Â·\frac{K_i(x)}{K_i(Î»_i+Îµ)}Â·\frac{(x-Î»_i)^{n_i}}{x-Î»_i-Îµ}\right)
$$

At this far, since $x$ appears in denomenator, we have no idea why this is a polynomial. 
\vfill
However, consider
$$
â€œConstâ€œ_Îµ\left(
\overbrace{
\underbrace{g(Î»_i+Îµ)}_{=a_0+a_1Îµ+\cdots}Â·{âˆ^{n_i-1}}Â·K_i(x)Â·\underbrace{\frac{1}{K_i(Î»_i+Îµ)}}_{=b_0+b_1Îµ+â€¦;}Â·\underbrace{\frac{Îµ^{n_i}}{x-Î»_i-Îµ}}_{=Îµ^{n_i}+\frac{Îµ^{n_i+1}}{x-Î»_i}+â€¦;}
}^{=a_0b_0K_i(x)Îµ+*Îµ^2+*Îµ^3+\cdots}\right)
=0
$$
\a\aa
Subtract one equation from another, you see that the whole term is indeed a polynomial.

$$ â€œInterpolation summandâ€œ= â£ â£ â£ â£ â£ â£ â£ â£ â£ â£ â£ â£ â£ â£ $$$$ 
â€œConstâ€œ_Îµ\left(g(Î»_i+Îµ)Â·{âˆ^{n_i-1}}Â·\frac{K_i(x)}{K_i(Î»_i+Îµ)}Â·\underbrace{\frac{(x-Î»_i)^{n_i}-Îµ^{n_i}}{x-Î»_i-Îµ}}_{â€œpolynomialâ€œ}\right)
$$


$$
â€œHintâ€œ:â£  \frac{A^n-B^n}{A-B}=A^{n-1}+A^{n-2}B+A^{n-3}B^2+\cdots+B^{n-1}
$$
\a\aa
\[defi]{
Define the polynomial (involving Îµ and âˆ as coefficients)
$$
f_{Î»_i}(x) = {âˆ^{n_i-1}}Â·\frac{K_i(x)}{K_i(Î»_i+Îµ)}Â·\frac{(x-Î»_i)^{n_i}-Îµ^{n_i}}{x-Î»_i-Îµ}
$$
and call it the \x{infinite interpolation polynomial at $Î»_i$ for $F(x)=(x-Î»_1)^{n_1}\cdots(x-Î»_k)^{n_k}$}
}
\[prop]{We have infinite Lagurange interpolation theroem
$$
g(x) = Q(x)F(x) + âˆ‘_{i=1}^k â€œConstâ€œ_Îµ(g(Î»_i+Îµ)f_{Î»_i}(x)).
$$
}


\a\aa
In the previous argument, we are using the idea that small enough scalars does not change the computation of constant part
$$
â€œConstâ€œ((a_nâˆ^n+â€¦+a_0+\underbrace{a_{-1}Îµ+â€¦}_{â€œdoes not affectâ€œ})(b_0+b_1Îµ+â€¦+b_nÎµ^n+\underbrace{b_{n+1}Îµ^{n+1}+â€¦}_{â€œdoes not affectâ€œ}))
$$
$$
=a_nb_n+a_{n-1}b_{n-1}+\cdots+a_0b_0.
$$
In other words, we do not need take care of all coefficient. 

\a{Big O notation}
To make the notation lighter, we write any terms like
$$
a_nÎµ^n+a_{n+1}Îµ^{n+1}+a_{n+2}Îµ^{n+2}\cdots  \sim O(Îµ^n)
$$
In other words, we may write both $Îµ+3Îµ^2$ and $Îµ+5Îµ^2+6Îµ^3$ as $Îµ+O(Îµ^2)$. The notation $O(Îµ^n)$ introduces some ambiguity.

The infinite scalar can be thought of some expression as 
$$
a_nâˆ^n+\cdots+a_0+O(Îµ).
$$
The multiplication of formal and infinite scalar has already determined up to $Îµ^n$ and nothing to do with $Îµ^{n+1}$
$$â€œConstâ€œ\left((a_nâˆ^n+\cdots+a_0+O(Îµ))(b_0+b_1Îµ+\cdots+b_nÎµ^n+O(Îµ^{n+1}))\right)$$
the result has determined and has nothing to do with $O(Îµ)$ of first factor and $O(Îµ^{n+1})$ of second factor.

\a{Algorithms of big O}
The big O notation follows the rule
$$ O(Îµ^n)\pm O(Îµ^m) \sim O(Îµ^{\min\{n,m\}}) $$
$$ O(Îµ^n)Â· O(Îµ^m) \sim O(Îµ^{n+m}) $$
\a\aa

%In the application, we only calculate expressions like
%$$
%â€œConstâ€œ((a_0+a_1Îµ+a_2Îµ^2+\cdots)(\cdots+M_{-1}Îµ+P_{Î»_i}+N_{Î»_i}âˆ+N_{Î»_i}^2âˆ^2+\cdots+N_{Î»_i}^{n_i-1}âˆ^{n_i-1}) )
%$$
%The result is $a_0P_{Î»_i}+a_1N_{Î»_i}+a_2N_{Î»_i}^2+\cdots+a_{n_{i}-1}N_{Î»_i}^{n_i-1}$, the matrix $M_{-1}$ have never been used, the infinitesimal part does not matter.
%\vfill
Intuitively, we don't care $O(Îµ)$ since it vanishes as $Îµ âŸ¶  0$, the infinitesimal part will have no influence of the limit at all.
\a\aa
\[defi]{We call 
$$
a_{-n}âˆ^n+a_{-n+1}âˆ^{n-1}+\cdots+a_0+O(Îµ)
$$
an \x{infinite scalar}, and when the context is clear, we simply write it as
$$
a_{-n}âˆ^n+a_{-n+1}âˆ^{n-1}+\cdots+a_0
$$
}
\a\aa

\[rem]{
Each Laurent scalar corresponds to a unique infinite scalar, we denote such correspondence by 
$$
â£ â£ a_{-n}âˆ^n+a_{-n+1}âˆ^{n-1}+\cdots+a_0+a_1Îµ+a_2Îµ^2+\cdots
$$
$$
 â‰¡  \;\;\; a_{-n}âˆ^n+a_{-n+1}âˆ^{n-1}+\cdots+a_0+O(Îµ) â£ â£ 
$$
}
\a\aa
\[defi]{
Two different Laurent scalars Î±, Î² might corresponds to the same infinite scalar. When this happens, we simply denote by Î± â‰¡ Î².
}
\[defi]{
Further abuse the notation, no matter Î± or Î² is infinite scalars or Laurent scalars, by Î± â‰¡ Î² we always mean they are the same as infinite scalars.
}
\[exa]{
$3âˆ+2+Îµ â‰¡ 3âˆ+2+3Îµ+2Îµ^2 â‰¡ 3âˆ+2+O(Îµ)$.
}
%\a\aa
%We introduce a equivilence relation:
%\vfill
%\[defi]{
% Two  Î± and Î² are equivalent $Î± â‰¡ Î²$ if $Î±-Î²$ is a formal scalar with constant equal to $0$
%$$
%a_{-n}âˆ^n+a_{-n+1}âˆ^{n-1}+\cdots+a_0+a_1Îµ+a_2Îµ^2+\cdots 
%$$
%$$
%â‰¡ 
%b_{-n}âˆ^n+b_{-n+1}âˆ^{n-1}+\cdots+b_0+b_1Îµ+b_2Îµ^2+\cdots 
%$$
%if and only if
%$$
%a_{-n}=b_{-n},\cdots,a_0=b_0.â£ â€œ but no restricitons to â€œa_1,b_1,a_2,\cdots
%$$
%The Laurant scalar, with infinitesimal part ignored, we call it the \x{infinite scalar}.
%}
%\a\aa
%\[rem]{Claming a infinite scalar introduced some ambiguity at first. For example, as an infinite scalar, 
%$$
%3âˆ+2
%$$
%could represent $3âˆ+2$, represent $3âˆ+2+Îµ$ or $3âˆ+2+3Îµ+2Îµ^2$. 
%}
\a\aa
Because infinite scalars has ambiguity of $O(Îµ)$, we \x{only allow infinite scalar to add or subtract } since $O(Îµ)Â±O(Îµ)=O(Îµ)$, \x{multiplication or division} is not allowed.
\vfill
Nevertheless, we can still multiply a formal scalar to an infinite scalar, the result is another infinite scalar.
$$
(2+3Îµ)(3âˆ+2) â‰¡ 6âˆ+13.
$$
\a\aa
$$ â€œ{\color{red} Infinite scalar} â€œ + â€œ{\color{red} Infinite scalar} â€œ â‰¡  â€œ{\color{red} Infinite scalar} â€œ $$
$$ â€œ{\color{red} Infinite scalar} â€œ - â€œ{\color{red} Infinite scalar} â€œ â‰¡  â€œ{\color{red} Infinite scalar} â€œ $$
$$ â€œ{\color{red} Infinite scalar} â€œ Ã—  â€œ{\color{red} Infinite scalar} â€œ = â€œNOT ALLOWED â€œ $$
$$ â€œ{\color{red} Infinite scalar} â€œ \div  â€œ{\color{red} Infinite scalar} â€œ = â€œNOT ALLOWED â€œ $$
$$â€œ{\color{blue} Formal scalar} â€œ+â€œ{\color{blue} Formal scalar} â€œ=â€œ{\color{blue} Formal scalar} â€œ$$
$$â€œ{\color{blue} Formal scalar} â€œ-â€œ{\color{blue} Formal scalar} â€œ=â€œ{\color{blue} Formal scalar} â€œ$$
$$â€œ{\color{blue} Formal scalar} â€œ Ã— â€œ{\color{blue} Formal scalar} â€œ=â€œ{\color{blue} Formal scalar} â€œ$$
$$â€œ{\color{blue} Formal scalar} â€œ \div\underbrace{â€œ{\color{blue} Formal scalar} â€œ}_{â€œnon-zeroâ€œ}=â€œ{\color{cyan} Laurent scalar} â€œ$$
$$ â€œ{\color{blue} Formal scalar} â€œ Ã— â€œ{\color{red} Infinite scalar} â€œâ‰¡ â€œ{\color{red} Infinite scalar} â€œ $$
$$ \underbrace{â€œ{\color{cyan} Laurent scalar} â€œ}_{â€œthat not a formal scalarâ€œ} Ã— â€œ{\color{red} Infinite scalar} â€œ=â€œNOT ALLOWEDâ€œ $$
\a{Degree of infinite scalar}
\[defi]{Call an infinite scalar 
$$
a_nâˆ^n+a_{n-1}âˆ^{n-1}+\cdots+a_0
$$
of degree $n$ if $a_nâ‰ 0$.
}
\a\aa
\[defi]{An infinite matrix is a matrix of infinite scalars. Define the degree of such matrix to be the maximal degree of its entries. }
\[defi]{An infinite vector is a $n Ã— 1$ infinite matrix.}
\[prop]{The set of $m Ã— n$ infinite matrix is closed under addition, subtraction, and formal scalar multiplication. But we can not define product of two infinite matrix!}
%\[prop]{Let $\vec v$ and $\vec u$ be infinite vectors, then $\vec v+\vec u$ and $\vec v-\vec u$ are all infinite vectors. For any formal scalar
%$$
%Î»=Î»_0+Î»_1Îµ+Î»_2Îµ^2+\cdots,
%$$
%the 
%$$
%Î»\vec v
%$$
%defines an infinite vector.}











\a\aa
Recall the infinite interpolation formula
$$
f_{Î»_i}(x) = {âˆ^{n_i-1}}Â·\frac{K_i(x)}{K_i(Î»_i+Îµ)}Â·\frac{(x-Î»_i)^{n_i}-Îµ^{n_i}}{x-Î»_i-Îµ}
$$
$$
g(x)=
Q(x)F(x) + âˆ‘_{i=1}^kâ€œConstâ€œ_Îµ\left(g(Î»_i+Îµ)Â·f_{Î»_i}(x)\right)
$$

Let $A$ be a matrix with $F(A)=0$,  we have specutral decomposition

\[prop]{
$$
g(A) = âˆ‘_{i=1}^kâ€œConstâ€œ_Îµ\left(g(Î»_i+Îµ)Â·ğ’«_{Î»_i}\right)
$$
where
$$
ğ’«_{Î»_i}:=f_{Î»_i}(A)
$$
}
\a\aa
Recall our settings
\[itemize]{
\item $F(x)=(x-Î»_1)^{n_1}\cdots(x-Î»_{k})^{n_k} = K_i(x)(x-Î»_i)^{n_i}$ for all $i$.
\item $F(A)=0$
\item $f_{Î»_i}(x) = {âˆ^{n_i-1}}Â·\frac{K_i(x)}{K_i(Î»_i+Îµ)}Â·\frac{(x-Î»_i)^{n_i}-Îµ^{n_i}}{x-Î»_i-Îµ}$
\item $ğ’«_{Î»_i}=f_{Î»_i}(A)$
}
\a\aa
Our goal in this lecture:

\[enumerate]{
\item We have
$$
A ğ’«_{Î»_i} â‰¡ ğ’«_{Î»_i}A â‰¡  (Î»_i+Îµ)ğ’«_{Î»_i}
$$
Later on we call such thing an \x{infinite eigenmatrix} of $A$ with eigenvalue $Î»_i+Îµ$.
\item We have $$
ğ’«_{Î»_i} â‰¡ P_{Î»_i}+N_{Î»_i}âˆ+N_{Î»_i}^2âˆ^2+\cdots+N_{Î»_i}^{n_i-1}âˆ^{n_i-1}
$$
for some  $P_{Î»_i}^2=P_{Î»_i}$, $N_{Î»_i}^{n_i}=0$ and $N_{Î»_i}P_{Î»_i}=P_{Î»_i}N_{Î»_i}$. Later on, we will call such thing \x{infinite projection matrix}.
\item We have
$$
P_{Î»_1}+P_{Î»_2}+\cdots+P_{Î»_k}= I.
$$
}

\a\aa
Let us prove the first, note that our goal is equivalent to show
$$
(A-(Î»_i+Îµ)I)ğ’«_{Î»_i}â‰¡  
ğ’«_{Î»_i}(A-(Î»_i+Îµ)I)â‰¡ 0.
$$
Note that
$$
\cancel{(x-(Î»_i+Îµ))}Â· \underbrace{{âˆ^{n_i-1}}Â·\frac{K_i(x)}{K_i(Î»_i+Îµ)}Â·\frac{(x-Î»_i)^{n_i}-Îµ^{n_i}}{\cancel{x-Î»_i-Îµ}}}_{f_{Î»_i}(x)}
$$
$$
= {âˆ^{n_i-1}}Â·\frac{K_i(x)Â·((x-Î»_i)^{n_i}-Îµ^{n_i})}{K_i(Î»_i+Îµ)}
$$
$$
=\frac{âˆ^{n_i-1}Â·\overbrace{K_i(x)Â·(x-Î»_i)^{n_i}}^{=F(x)}}{K_i(Î»_i+Îµ)}
-\overbrace{\frac{K_i(x)Â·âˆ^{n_i-1}Â·Îµ^{n_i}}{K_i(Î»_i+Îµ)}}^{â‰¡ 0}
$$
\a\aa

{\bf Conclusion}: $(x-(Î»_i+Îµ))f_{Î»_i}(x) â‰¡ âˆ^{n_i-1}F(x)/K_i(Î»_i+Îµ)$
\vfill
Since $F(A)=0$, $ğ’«_{Î»_i}=f_{Î»_i}(A)$, plug in $x=A$, 

$$(A-(Î»_i+Îµ))f_{Î»_i}(A) â‰¡ âˆ^{n_i-1}F(A)/K_i(Î»_i+Îµ) = âˆ^{n_i-1} 0/K_i(Î»_i+Îµ) = 0 $$
we finish the proof.


\a\aa

\[defi]{We call an infinite square matrix $ğ’«$ an \x{infinite eigenmatrix} of eigenvalue $Î»+Îµ$ of $A$ if 
$$
A ğ’« â‰¡ ğ’« A â‰¡ (Î»+Îµ)ğ’« .
$$}
\[defi]{We call a \x{non-zero} infinite vector $\vec v$ the \x{infinite eigenvector} of eigenvalue $Î»+Îµ$ if 
$$
A\vec v  â‰¡  (Î»+Îµ)\vec v.
$$}
\a\aa
Example.
$$
A = \m11,01., â£ 
\vec v â‰¡ \m{âˆ},1.
$$
is an infinite eigenvector of eigenvalue $1+Îµ$. Indeed
$$
A\vec v â‰¡ \m11,01.\m{âˆ},1. â‰¡  \m{âˆ+1},1.  â‰¡  (1+Îµ)Â·\m{âˆ},1.
$$

\a\aa
Infinite eigenvector are generalizations of eigenvector:
\[rem]{\[itemize]{
\item A (classical) eigenvector is an infinite eigenvector of degree 0.
\item A constant matrix $A$ has a \x{non-classical infinite eigenvector} of eigenvalue $Î»+Îµ$ ($Î» âˆˆ â„‚ $) if and only if it is \x{non-digaonalizable}.(Try to prove it yourself.)
}
}
\a\aa
Scaling property
\[prop]{Any formal scalar multiple of infinite eigen vector $Î¼\vec v$ is either zero vector or an infinite eigenvector of the same eigenvalue.  }

$$
A\vec v = (Î»+Îµ)\vec v âŸ¹  AÎ¼\vec v=Î¼A\vec v = Î¼(Î»+Îµ)\vec v=(Î»+Îµ)Î¼\vec v
$$

\a\aa

 Example:
$$
\m11,01.\m{âˆ},1. â‰¡ (1+Îµ)\m{âˆ},1.   â£  â€œInfinite eigenvector of degree 1â€œ
$$
\vfill
$$ \m11,01.\m1,0.  â‰¡  (1+Îµ)\m1,0. â£  \substack{â€œClassical eigenvector;â€œ\\â€œInfinite eigenvector of degree 0â€œ} $$
\vfill
Relation of these two infinite eigenvectors
 $$ \m1,0. â‰¡ \underbrace{Îµ}_{\substack{â€œformalâ€œ\\â€œscalarâ€œ\\â€œmultipleâ€œ}}\m{âˆ},1.  $$
\a\aa
The appearance of infinite eigenvectors indicates that there are multiple eigenspaces repeated to each other. And the infinite eigenvector have some cosins.
\vfill
\textbf{Philosophy}
Recall the theory of interpolation polynomials, the appearance of interpolation polynomial 
$$f_{Î»_i}(x)=*âˆ^{n_i-1}+*âˆ^{n_i-2}+â€¦+*âˆ+*$$ indicates the existence of repeated root of multiplicity $n_i$, and $f_{Î»_i}$ has $n_i-1$ other cosins, when summing them up, \x{the infinity cancels} and it gives a finite value $â€œConstâ€œ(f_{Î»_i})$.
\a\aa
How to understand the infinite eigenvector ? Let's introduce a variable $a$.
$$
\m{1+a}1,01.
$$
When $a=0$, the matrix is NOT diagonalizable and results an infinite eigenvector
$$
\m11,01.\m{âˆ},1.=(1+Îµ)\m{âˆ},1.
$$
However, \x{when $aâ‰ 0$}, the matrix is \x{always diagonalizable} and admits a spectural decomposition
$$
g\m{1+a}1,01. = g(1+a)\m1{\frac1a},00.+g(1)\m0{-\frac1a},01..
$$
\a\aa
To visualize the change of eigenvector along $a$, we fix a vector and look its eigenvector decomposition (which is unique)
$$
\m 0,1. = \underbrace{\m{\frac1a},0.}_{\substack{â€œeigenvalueâ€œ\\=1+a}}+\underbrace{\m{-\frac1a},1.}_{\substack{â€œeigenvalueâ€œ\\=1}}
$$
\a\aa
Let's visualize what happens when $a âŸ¶ 0$ The blue vector is our fixed vector, and red vector is its decomposition as eigenvectors
\vfill

This is what happend for $a=0.1$

\[z]{[scale=0.2]
\draw[->,ultra thick,blue](0,0)--(0,1);
\draw[->,ultra thick,red](0,0)--(-10,1);
\draw[->,ultra thick,red](0,0)--(10,0);
}

This is what happend for $a=0.05$

\[z]{[scale=0.2]
\draw[->,ultra thick,blue](0,0)--(0,1);
\draw[->,ultra thick,red](0,0)--(-20,1);
\draw[->,ultra thick,red](0,0)--(20,0);
}

This is what happend for $a=0.03$

\[z]{[scale=0.2]
\draw[->,ultra thick,blue](0,0)--(0,1);
\draw[->,ultra thick,red](0,0)--(-30,1);
\draw[->,ultra thick,red](0,0)--(30,0);
}

When $a=0$, \x{eigenvectors goes to infinities} in opposite direction. \x{But the sum} of this two eigenvector \x{is a finite vector}, which is our fixed one.

\[z]{[scale=0.2]
\draw[->,ultra thick,blue](0,0)--(0,1);
\draw[->,ultra thick,red](-30,0)--(30,0);
}
\a\aa
The supper big red vector is described by infinite eigenvector
$$
\m{âˆ},1.
$$
\[z]{[scale=0.2]
\draw[->,ultra thick,blue](0,0)--(0,1);
\draw[->,ultra thick,red](-30,0)--(30,0);
}

The sum of these two supper big eigenvector, is described as what we known as the constant part
$$
â€œConstâ€œ\m{âˆ},1.= \m0,1.
$$
\a\aa
From the perspective of eigenspace, as $a âŸ¶ 0$, the eigenspaces are running close to each other.

This is what happend for $a=0.1$

\[z]{[scale=0.2]
\draw[thick,orange](-30,-3)--(30,3);
\draw[thick,orange](-30,0)--(30,0);
}

This is what happend for $a=0.05$

\[z]{[scale=0.2]
\draw[thick,orange](-30,-1.5)--(30,1.5);
\draw[thick,orange](-30,0)--(30,0);
}

This is what happend for $a=0.03$

\[z]{[scale=0.2]
\draw[thick,orange](-30,-1)--(30,1);
\draw[thick,orange](-30,0)--(30,0);
}

This is what happend for $a=0.01$

\[z]{[scale=0.2]
\draw[thick,orange](-30,-0.3)--(30,0.3);
\draw[thick,orange](-30,0)--(30,0);
}


\a\aa
We now answer the second question, determine the structure of $ğ’«_{Î»_i} $.

Recall that
$$
f_{Î»_i}(x) = {âˆ^{n_i-1}}Â·\frac{K_i(x)}{K_i(Î»_i+Îµ)}Â·\frac{(x-Î»_i)^{n_i}-Îµ^{n_i}}{x-Î»_i-Îµ},
$$
and
$$
ğ’«_{Î»_i}:=f_{Î»_i}(A).
$$
the coefficients of $f_{Î»_i}$ has âˆ-degree at most $n_i-1$.

Therefore, ğ’«_{Î»_i} is a matrix with âˆ-degree at most $n_i-1$, we have

$$
ğ’«_{Î»_i} â‰¡ P_0+N_1âˆ+N_2âˆ^2+â€¦+N_{n_i-1}âˆ^{n_i-1}
$$
for some constant matrices $P_0,N_1,â€¦,N_{n_i-1}$. Now we determine the properties of these matrices.






\a\aa
Using spectral decomposition,  we obtain two expressions for $g(A)h(A)$
$$
\underbrace{âˆ‘_{i=1}^kâ€œConstâ€œ_Îµ\left(g(Î»_i+Îµ)Â·h(Î»_i+Îµ)Â·ğ’«_{Î»_i}\right)}_{g(A)h(A)}
=
$$
$$\underbrace{\left(
âˆ‘_{i=1}^kâ€œConstâ€œ_Îµ\left(g(Î»_i+Îµ)Â·ğ’«_{Î»_i}\right)
\right)}_{g(A)}
Â·
\underbrace{\left(
âˆ‘_{i=1}^kâ€œConstâ€œ_Îµ\left(h(Î»_i+Îµ)Â·ğ’«_{Î»_i}\right)
\right)}_{h(A)}
$$
\a\aa
Let Î±,Î² be two arbitrary formal scalars, put
\t
{}{$Î»_1+Îµ$}{$Î»_2+Îµ$}{$\cdots$}{$Î»_i+Îµ$}{$\cdots$}{$Î»_k+Îµ$},
{$g(x)$}{$0+O(Îµ^{n_1})$}{$0+O(Îµ^{n_2})$}{$\cdots$}{$Î±+O(Îµ^{n_i})$}{$\cdots$}{$0+O(Îµ^{n_k})$},
{$h(x)$}{$0+O(Îµ^{n_1})$}{$0+O(Îµ^{n_2})$}{$\cdots$}{$Î²+O(Îµ^{n_i})$}{$\cdots$}{$0+O(Îµ^{n_k})$},
{$g(x)h(x)$}{$0+O(Îµ^{n_1})$}{$0+O(Îµ^{n_2})$}{$\cdots$}{$Î±Î²+O(Îµ^{n_i})$}{$\cdots$}{$0+O(Îµ^{n_k})$}.

From the table,
$$ g(A)= âˆ‘_{i=1}^kâ€œConstâ€œ_Îµ\left(g(Î»_i+Îµ)Â·ğ’«_{Î»_i}\right) =â€œConstâ€œ_Îµ\left(Î±Â·ğ’«_{Î»_i}\right) $$
$$ h(A)= âˆ‘_{i=1}^kâ€œConstâ€œ_Îµ\left(h(Î»_i+Îµ)Â·ğ’«_{Î»_i}\right) =â€œConstâ€œ_Îµ\left(Î²Â·ğ’«_{Î»_i}\right) $$
$$ g(A)h(A)= âˆ‘_{i=1}^kâ€œConstâ€œ_Îµ\left(g(Î»_i+Îµ)h(Î»_i+Îµ)Â·ğ’«_{Î»_i}\right) =â€œConstâ€œ_Îµ\left(Î±Î²Â·ğ’«_{Î»_i}\right) $$

%We call $ğ’«_{Î»_i}$ the infinite eigenspace projection of eigenvalue $Î»_i+Îµ$.

\a\aa
The above argument proves that
$$
â€œConstâ€œ_Îµ\left(Î±Â·ğ’«_{Î»_i}\right)â€œConstâ€œ_Îµ\left(Î²Â·ğ’«_{Î»_i}\right)=â€œConstâ€œ_Îµ\left(Î±Î²Â·ğ’«_{Î»_i}\right)
$$
for any formal scalar Î±,Î².

\[defi]{We call an infinite matrix ğ’« an \x{infinite projection} matrix, if 
$$
Const(xğ’«)
Const(yğ’«)
=
Const(xyğ’«)
$$
for any \x{formal scalar} $x,y$
}

\a\aa
\[defi]{A constant matrix $P$ is called \x{projection} if $P^2=P$. }
\[rem]{To see analogue with infinite projection, $P^2=P$ is equivalent to  $(aP)(bP)=(abP)$.}
\[defi]{A constant matrix $N$ is called \x{nilpotent} if $N^k=0$ for some integer $k$.}
\a\aa
\[thm]{An infinite matrix $ğ’«$ of degree at most $m-1$ is an infinite projection matrix if and only if
$$
ğ’« =P+Nâˆ+N^2âˆ^2+\cdots+N^{m-1}âˆ^{m-1}
$$
with $P^2=P$, $PN=NP=N$, and $N^m=0$.
}
{\bf Proof}: If ğ’« is an infinite projection, assume
$$
ğ’«=P+Nâˆ+N_2âˆ^2+\cdots+N_{m-1}âˆ^{m-1}
$$
$$
PP=â€œConstâ€œ(1Â·ğ’«)â€œConstâ€œ(1Â·ğ’«)=â€œConstâ€œ(1Â·1Â·ğ’«)=P.
$$
$$PN=â€œConstâ€œ(1Â·ğ’«)â€œConstâ€œ(ÎµÂ·ğ’«)=â€œConstâ€œ(1Â·ÎµÂ·ğ’«)â€œ=Constâ€œ(ÎµÂ·ğ’«)=N.  $$
$$NP=â€œConstâ€œ(ÎµÂ·ğ’«)â€œConstâ€œ(1Â·ğ’«)=â€œConstâ€œ(ÎµÂ·1Â·ğ’«)â€œ=Constâ€œ(ÎµÂ·ğ’«)=N.  $$
$$N_k=â€œConstâ€œ(Îµ^kğ’«) = â€œConstâ€œ(Îµğ’«)^k = N^k âŸ¹   N_k=N^k$$
$$
N^m = â€œConstâ€œ(ÎµÂ·ğ’«)^m = â€œConstâ€œ(Îµ^mÂ·ğ’«) = 0.
$$
\a\aa
\textbf{Proof of the other direction}: Suppose $P^2=P$, $PN=NP=N$, and $N^m=0$, and 
$$
ğ’« =P+Nâˆ+N^2âˆ^2+\cdots+N^{m-1}âˆ^{m-1}
$$
To prove 
$$
â€œConstâ€œ(\underbrace{(a_0+a_1Îµ+â€¦+O(Îµ^m))}_{Î±}ğ’«)
â€œConstâ€œ(\underbrace{(b_0+b_1Îµ+â€¦+O(Îµ^m))}_{Î²}ğ’«)
$$
$$
=
â€œConstâ€œ(\underbrace{(a_0+a_1Îµ+â€¦+O(Îµ^m))(b_0+b_1Îµ+â€¦+O(Îµ^m))}_{Î±Î²}ğ’«)
$$
It suffices to prove
$$
â€œConstâ€œ(Îµ^k ğ’« )
â€œConstâ€œ(Îµ^n ğ’« )=
â€œConstâ€œ(Îµ^{k+n} ğ’« ) â£ 
$$
for $m,nâ‰¥0$. This is the same as proving
$$
P^2=P, PN^n = N^n, N^kP=N^k, N^kN^n=N^{k+n}, N^m=0,
$$
which is obviously true.

\a\aa
\[rem]{A classical projection is an infinite projection $ğ’«=P$ 
$$
ğ’« =P+Nâˆ+N^2âˆ^2+\cdots+N^{m-1}âˆ^{m-1}
$$
with the case $N=0$.
}
Therefore, the appearence of non-zero nilpotent matrix indicates the eigenspace projection could go to infinity, making non-classical infinite eigenvectors possible, and therefore implies the matrix non-diagonalizable.
\a\aa

Recall that when $A$ satisfies $(A-Î»_1)^{n_1}â€¦(A-Î»_k)^{n_k}=0$, our method to check diagonalizability of $A$ is by checking whether 
$$(A-Î»_1)â€¦(A-Î»_k)=0.$$ 
\vfill
The reason behind it is because that
$$
(A-Î»_1)^{n_1}â€¦(A-Î»_k)^{n_k}=0 âŸ¹  (A-Î»_1)â€¦(A-Î»_k) â€œ nilpotentâ€œ.
$$
\vfill
Intuitively, if the nilpotent part $=0$, we were in the situation that no eigenspace projection goes to infinity, and therefore the matrix is diagonalizable.
\a\aa
The philosophy that nilpotent matrix is the infinite part of infinite projection seems deep. Let's consider this.

\vfill
It is true that
$$
\frac{wv^T}{v^Tw}
$$
is always a rank 1 projection matrix. What happens if $v^Tw=0$? Then the denomenator is $0$, the whole matrix goes to infinity. However, when $v^Tw=0$, the matrix $wv^T$ must be nilpotent since
$$
(wv^T)^2=wv^Twv^T = w\underbrace{(v^Tw)}_{=0}v^T=0.
$$
Therefore, when $v^Tw=0$, you may view this as a 
$$
\underbrace{\frac1{v^Tw}}_{\substack{â€œscalarâ€œ\\â€œroughly â€œâˆ}}Â·\underbrace{wv^T}_{â€œnilpotent matrixâ€œ}
$$
This gives you an idea why nilpotency is an inifnite part of infinite projection.
\a\aa
Sun set: Please visualize infinite projection $ğ’« $ in the following picture.

\[tikzpicture]{[scale=0.4]
\draw[ultra thick](-7,0)--(7,0);
\draw[ultra thick,->,blue](0,0)--(0,3) node[right]{$\vec v=\m0,1.$};
\draw[opacity=0.08,fill=cyan] (0,0)--(10,0)--(10,3)--(0,3)--(0,0);
\draw[ultra thick,cyan](0,0.05)--(7,0.05) node[below]{infinite shadow}--(15,0.05) ;
\foreach\x in {0,0.5,...,4}{
	\draw [red,->](-8,\x)--(8,\x);
}
\draw[fill=red] (0,0) circle[radius=0.1] node[below]{$O$};
}

The vector $\m1,0.$ is on the groud, so its shadow is itself. We have
$$
ğ’«\m1,0. = \m1,0.
$$

The vector $\m0,1.$ stands up, and its shadow goes to infinity, so
$$
ğ’«\m0,1. = \m{âˆ},1.
$$
\a\aa
Therefore, this infinite projection takes the form

$$
ğ’« = ğ’« \m 10,01. = \m 1{âˆ},01. = \underbrace{\m10,01.}_{â€œprojection matrixâ€œ}+âˆ\underbrace{\m01,00.}_{â€œnilpotent matrixâ€œ}.
$$

\a\aa
\exe For $ğ’«_{Î»_i}=P_{Î»_i}+N_{Î»_i}âˆ+â€¦+N_{Î»_i}^{n_i-1}âˆ^{n_i-1}$. What is the relation between $A$, $P_{Î»_i}$ and $N_{Î»_i}$?
\vfill
\sol $Ağ’«_{Î»_i}=(Î»_i+Îµ)ğ’« $

So $(A-Î»_i)ğ’«_{Î»_i}=Îµ ğ’«_{Î»_i}$. Taking the constant part on both sides,
$$
(A-Î»_iI)â€œConstâ€œ(ğ’«_{Î»_i})=â€œConstâ€œ(Îµ ğ’«_{Î»_i})
$$
so 
$$
(A-Î»_iI)P_{Î»_i}=N_{Î»_i}
$$
\aaa



\aaa{Infinite eigenvector decomposition}
We have left the last property
$$
P_{Î»_1}+
P_{Î»_2}+
â€¦
+P_{Î»_k}=I.
$$
This is easist property by taking $g(x)=1$, then
$$
I = g(A) = âˆ‘_{i=1}^k (g(Î»_i+Îµ)ğ’«_{Î»_i})=
âˆ‘_{i=1}^k (ğ’«_{Î»_i})
=âˆ‘_{i=1}^k P_{Î»_i}.
$$
\a\aa
\[thm]{
For $(A-Î»_1)^{n_1}â€¦(A-Î»_k)^{n_k}=0$, there are infinite-matrix $ğ’«_{Î»_i}$ with
\[enumerate]{
\item $ğ’«_{Î»_i}$ is infinite eigenmatrix of eigenvalue $Î»_i+Îµ$
$$
A ğ’«_{Î»_i} â‰¡ ğ’«_{Î»_i}A â‰¡  (Î»_i+Îµ)ğ’«_{Î»_i}
$$
\item $ğ’«_{Î»_i}$ is infinite projection
$$
ğ’«_{Î»_i} â‰¡ P_{Î»_i}+N_{Î»_i}âˆ+N_{Î»_i}^2âˆ^2+\cdots+N_{Î»_i}^{n_i-1}âˆ^{n_i-1}
$$
\item We have
$$
P_{Î»_1}+P_{Î»_2}+\cdots+P_{Î»_k}= I.
$$
\item We have for any polynomial $g$,
$$
g(A)=âˆ‘_{i=1}^kâ€œConstâ€œ(g(Î»_i)ğ’«_{Î»_i}).
$$
}
}
\a\aa
In this lecture, we introduce two big picture:

\[itemize]{
\item \x{Infinite eigenvector decomposition}: Decomposing every vector into infinite eigenvectors.
\item \x{Infinite eigenbasis}: Fix some infinite eigenvectors so that every vector can be represents as their linear combination.
}

\a{Review classical decomposition}
We review eigenvector decomposition in diagonalizable cases. Let us just say a matrix
$$
A = \m
{Î»_1}{}{}{},
{}{Î»_1}{}{},
{}{}{Î»_2}{},
{}{}{}{Î»_3}.
$$
with $Î»_1â‰ Î»_2â‰ Î»_3â‰ Î»_1$.
Then every vector can be decomposed into eigenvectors of $A$
$$
\m a,b,c,d. = 
\underbrace{\m a,b,0,0. }_{\substack{â€œeigenvalueâ€œ\\=Î»_1}}
+\underbrace{\m0,0,c,0. }_{\substack{â€œeigenvalueâ€œ\\=Î»_2}}
+ \underbrace{\m0,0,0,d.}_{\substack{â€œeigenvalueâ€œ\\=Î»_3}}
$$
This is eigenvector decomposition.
\a\aa
However, eigenbasis means a fixed eigen vector, and we are writing any vector into linear combination of them. For example, the list
$$
\underbrace{\m 1,0,0,0.}_{\substack{â€œeigenvalueâ€œ\\=Î»_1}} , â£ 
\underbrace{\m 0,1,0,0.}_{\substack{â€œeigenvalueâ€œ\\=Î»_1}} , â£ 
\underbrace{\m 0,0,1,0.}_{\substack{â€œeigenvalueâ€œ\\=Î»_2}} , â£ 
\underbrace{\m 0,0,0,1.}_{\substack{â€œeigenvalueâ€œ\\=Î»_3}}  
$$
is an eigenbasis and we gonna write any vector by linear combination of them
$$
\m a,b,c,d. = 
\m 1,0,0,0.a +
\m 0,1,0,0.b +
\m 0,0,1,0.c +
\m 0,0,0,1.d 
$$
\a\aa
For non-diagonalizable matrices, its anihilating polynomial have repeated roots. We imagine its roots come together as clusters.

\[z]{
\draw (0,0)--(10,0);
\draw[fill=red] (0,0) circle[radius=0.1];
\draw[fill=red] (0.2,0) circle[radius=0.1];
\draw[fill=red] (0.4,0) circle[radius=0.1];
\draw[fill=red] (7,0) circle[radius=0.1];
\draw[fill=red] (7.2,0) circle[radius=0.1];
\draw[fill=red] (3,0) circle[radius=0.1];
\draw[fill=red] (3.2,0) circle[radius=0.1];
\draw[fill=red] (3.4,0) circle[radius=0.1];
}

It is hard to seperate them, so we should not consider just one eigenvalue, consider them together. 
\a\aa
Let's come back tO easieR diAgonalizaBle CAsEs
\[z]{
\draw (0,0)--(10,0);
\draw[fill=red] (0,0) circle[radius=0.1];
\draw[fill=red] (7,0) circle[radius=0.1];
\draw[fill=red] (3,0) circle[radius=0.1];
}

\a\aa
In the above decomposition, 
$$
\m a,b,c,d. = 
\underbrace{\m a,b,0,0. }_{\substack{â€œeigenvalueâ€œ\\=Î»_1}}
+\underbrace{\m0,0,c,0. }_{\substack{â€œeigenvalueâ€œ\\=Î»_2}}
+ \underbrace{\m0,0,0,d.}_{\substack{â€œeigenvalueâ€œ\\=Î»_3}}
$$

We can say 
$$ \m a,b,0,0.   â£ â€œthe â€œÎ»_1-â€œ component â€œ â£  \m 0,0,c,0.   â£ â€œthe â€œÎ»_2-â€œ component â€œ â£ $$$$ \m 0,0,0,d.   â£ â€œthe â€œÎ»_3-â€œ component â€œ $$
\a\aa
$$
\m a,b,c,0. â£ â€œtheâ€œ (Î»_1,Î»_2)-â€œcomponentâ€œ â£ 
\m a,b,0,d. â£ â€œtheâ€œ (Î»_1,Î»_3)-â€œcomponentâ€œ
$$

$$
\m 0,0,c,d. â£ â€œtheâ€œ (Î»_2,Î»_3)-â€œcomponentâ€œ â£ 
\m a,b,c,d. â£ â€œtheâ€œ (Î»_1,Î»_2,Î»_3)-â€œcomponentâ€œ
$$
Note that multivalue component are no longer eigenvectors!
\a\aa
The idea of multi-eigenvalue component is that when eigenvalue are given 
$$
(Î»_1,Î»_2,â€¦,Î»_n)
$$
Any partitioN oF the above list would give a decomposition, where $(Î»_i,Î»_j,...,Î»_k)$-component means a vector $\vec w$ with
$$
(A-Î»_iI)(A-Î»_jI)â€¦(A-Î»_kI)\vec w=\vec 0.
$$
\a\aa
$Î»_1$-component means $(A-Î»_1I)\vec v=\vec 0$


$(Î»_1,Î»_2)$-component means $(A-Î»_1I)(A-Î»_2I)\vec v=\vec 0$
\a\aa

For $ğ’«_{Î»_i}=P_{Î»_i}+N_{Î»_i}âˆ+â€¦+N_{Î»_i}^{n_i-1}âˆ^{n_i-1}$.

Any classical vector $\vec v$ can be decomposed into
$$
\vec v = I\vec v=
P_{Î»_1}\vec v+
P_{Î»_2}\vec v+
â€¦+
P_{Î»_k}\vec v.
$$

Note that
$$
(A-Î»_iI)^{n_i}P_{Î»_i}=(A-Î»_iI)^{n_i}â€œConstâ€œ(ğ’«_{Î»_i})
=â€œConstâ€œ((A-Î»_iI)^{n_i}ğ’«_{Î»_i})
$$
$$
=â€œConstâ€œ(Îµ^{n_i}ğ’«_{Î»_i})=0.
$$

Therefore, 
$$
P_{Î»_i}\vec v
$$
is an $(\underbrace{Î»_i,Î»_i,â€¦Î»_i}_{n_iâ€œmanyâ€œ})$-component of $\vec v$
\a\aa
Similar to diagonalizable cases, in the decomposition 
$$
\vec v = I\vec v=
P_{Î»_1}\vec v+
P_{Î»_2}\vec v+
â€¦+
P_{Î»_k}\vec v.
$$
\vfill
Each $P_{Î»_i}\vec v$ is not an eigenvector if $n_iâ‰¥2$. Instead, it represents the sum of certain eigenvectors. In the non-diagonalizable cases, it can be viewed as the sum of infinite eigenvectors, and it is finite since the infinite part has been canceled. 
\a\aa
To decompose further, we may see
$$
\underbrace{P_{Î»_i}\vec v}_{\substack{(\underbrace{Î»_i,Î»_i,â€¦,Î»_i}_{n_i-â€œmanyâ€œ})-â€œcomponentâ€œ\\â€œNot an eigenvector if â€œn_iâ‰¥2}} = \overbrace{â€œConstâ€œ(\underbrace{ğ’«_{Î»_i}\vec v}_{\substack{Î»_i-â€œcomponentâ€œ\\â€œeigenvectorâ€œ}})}^{â€œthe sum of each â€œÎ»_i-component}
$$
\vfill

Taking constant part of an infinite vector is analogue of summing up infinite eigenvector of \x{the same, but infinitesimally different} eigenvalues.  This euqaion represents the decomposition of $(Î»_i,Î»_i,â€¦,Î»_i)$-component to $Î»_i$-component, which is a true infinite eigenvector.
\a\aa
Therefore, we regard
$$
\vec v = â€œConstâ€œ(ğ’«_{Î»_1}\vec v)+â€œConstâ€œ(ğ’«_{Î»_2}\vec v)+â€¦+â€œConstâ€œ(ğ’«_{Î»_k}\vec v)
$$
as the \x{infinite eigenvector decomposition} of any vector $\vec v$.

\a\aa
Review In diagonalization theory, we also have a theorem
\[thm]{Suppose $A$ is diagonalizable matrix, eigenvectors with different eigenvalues must be linearly independent.
}
The proof is easy, since we have spectural decomposition
$$
P_{Î»_1}+P_{Î»_2}+â€¦+P_{Î»_k}=I  
$$
Suppose $A\vec v=Î»_i\vec v    âŸ¹  P_{Î»_j}\vec v =\vec 0 $ for $iâ‰ j$ because the construction of $P_{Î»_j}$ has the factor of $(A-Î»_iI)$. 
Therefore only $ P_{Î»_i}\vec v=I\vec v=\vec v$.
\vfill
Suppose $\vec v_{Î»_1},â€¦,\vec v_{Î»_k}$ are eigenvectors of eigenvalues $Î»_1,â€¦Î»_k$. 
$$
a_1\vec v_{Î»_1}+a_2\vec v_{Î»_2}+â€¦+a_k\vec v_{Î»_k}=\vec 0.
$$
$$
a_i\vec v_{Î»_i} = P_{Î»_i}\vec 0=\vec 0. âŸ¹  a_i=0.
$$
\a\aa
To summarise, if $A\vec v_{Î»_i}=Î»_i\vec v_{Î»_i}$, then
$$
a_1\vec v_{Î»_1}+\cdots+a_k\vec v_{Î»_k}=\vec 0 âŸ¹   a_i\vec v_{Î»_i}=\vec 0 â€œ for all â€œi.
$$ 

\a\aa
We carry out the similar situation. 

\[lem]{Suppose $\vec v$ is an infinite eigenvector of $A$ with eigenvalue $Î»+Îµ$, then 
$$
â€œConstâ€œ(\vec v)=0 âŸº   \vec v â‰¡ 0.
$$}
{\textbf{Understanding(not proof)}}:$â€œConstâ€œ(\vec v)$ may be understood as the sum of the inifinte eigenvector with its friends, different friend have slightly different eigenvalue, so the sum is zero implies each individual is zero.
\a\aa
\textbf{Proof}: If $\vec v â‰¡ 0$, then clearly $â€œConstâ€œ(\vec v)=0$.
\vfill
On the contary, suppose $\vec v=\vec v_0+\vec v_1âˆ+\vec v_2âˆ^2+â€¦$
$$
\vec v_0=â€œConstâ€œ(\vec v)=\vec 0   $$
$$ âŸ¹  \vec v_n=â€œConstâ€œ(Îµ^n\vec v)=â€œConstâ€œ((A-Î»I)^n\vec v) = (A-Î»I)^nâ€œConstâ€œ(\vec v)$$

$$=(A-Î»I)^n\vec 0=\vec 0.
$$

Therefore $\vec v = \vec 0+\vec 0âˆ+\vec 0âˆ^2+\cdots=\vec 0$.

\a\aa
Now we proceed to \x{infinite eigenbasis}. 
\[defi]{An \x{infinite eigenbasis} is a list of infinite eigenvectors $$\vec w_{Î»_1,1},â£ \vec w_{Î»_1,2} â£ â€¦,â£  \vec w_{Î»_k,s_k} ,$$ such that for all classical vector $\vec v$, \x{there exists } formal scalars $a_{Î»_1,1},â€¦,a_{Î»_k,s_k}$ and \x{unique} $a_{Î»_1,1}\vec w_{Î»_1,1},â€¦,a_{Î»_k,s_k}\vec w_{Î»_k,s_k}$ such that we may represent $\vec v$ as the constant part of a linear combination of fixed those vectors
$$
\vec v = â€œConstâ€œ(a_{Î»_1,1}\vec w_{Î»_1,1})+â€œConstâ€œ(a_{Î»_1,2}\vec w_{Î»_1,2})+â€¦+â€œConstâ€œ(a_{Î»_k,s_k}\vec w_{Î»_k,s_k})
$$
}
\vfill
In this part, we will give algorithm on finding those $\vec w_{Î»_1,1}$. Again, when finding a basis, we use \x{cross-filling}. 
\a\aa
We say unique $a\vec w$ instead of unique $a$ since $\vec w$ is an infinite vector, and $a$ is a formal scalar, we would view $a,b$ equivalent when $a\vec w â‰¡ b\vec w$. Therefore, it is more accurate to say unique $a\vec w$ instead of unique $a$.
\vfill
For examle, 
$$
(3+Îµ)\m{âˆ},2. â‰¡ (3+Îµ+5Îµ^2)\m{âˆ},2.
$$
but as a formal scalar $3+Îµâ‰ 3+Îµ+5Îµ^2$.
\a\aa
To obtain such a basis, we apply cross-filling to the infinite matrix.


%%% Example of cross-filling



\a\aa
\[defi]{Let $\vec v_1,\cdots,\vec v_n$ be a list of \x{non-zero} infinite vectors. We say they are \x{linearly independent} if any linear combination of formal scalars
$$
a_1\vec v_1+a_2\vec v_2+\cdots+a_n\vec v_n â‰¡ \vec 0
$$
implies
$$
a_1\vec v_1 â‰¡ a_2\vec v_2 â‰¡ \cdots â‰¡ a_n\vec v_n â‰¡ 0.
$$
}

\a\aa
when cross-filling infinite matrix, the cross-center must choose at the largest degree
$$
\m{2âˆ-1}{2âˆ}{âˆ},
{âˆ^2}{âˆ^2}{âˆ^2},
{âˆ}{âˆ}1.
$$

$$
=\m
{âˆ}{âˆ}{\yâˆ},
{\yâˆ^2}{\yâˆ^2}{\eâˆ^2},
11\1.+
\m{\yâˆ-1}{\eâˆ}\0,0\00,{âˆ-2}{\yâˆ-1}0. +\m\000,\000,{\e1}\0\0.
$$
%% Demonstration that vectors from cross-filling are linearly independent.
\a\aa
The cross cOlumns must be lInEARly iNDEpenDENt
$$
a_1\m{\yâˆ},{\eâˆ^2},{\1}.
+
a_2\m{\eâˆ},{\y0},{\yâˆ-1}.
+a_3\m\0,\0,{\e1}. â‰¡ 0
$$
by looking at the rows from the first cross center, 
$$
a_1âˆ^2 â‰¡ 0 âŸ¹   a_1 ~ O(Îµ^2) âŸ¹   a_1\m{\yâˆ},{\eâˆ^2},{\1}. â‰¡ \m0,0,0.
$$
\a\aa
Therefore
$$
a_2\m{\eâˆ},{\y0},{\yâˆ-1}.
+a_3\m\0,\0,{\e1}. â‰¡ 0
$$
Looking at the first cross centerâ£ $a_2âˆ=0$ so
$$
a_2\m{\eâˆ},{\y0},{\yâˆ-1}. â‰¡ \m0,0,0.
$$
Inductively, we also show
$$
a_3\m\0,\0,{\e1}. â‰¡ 0.
$$
This inductively shows that \x{the cross-filling will always produce linearly independent vectors.} In fact, it produces basis for the column space of the matrix.
\a\aa
\[defi]{
For an infinite vector
$$
\vec v = \vec v_0 + \vec v_1âˆ+\vec v_2âˆ^2+\cdots+\vec v_mâˆ^m
$$
we call $\vec v_0$, $\vec v_1$, $â€¦$, $\vec v_m$ its components.
}
\a\aa
\[lem]{Suppose infinite vectors $\vec w_1,\vec w_2,â€¦,\vec w_k$ is a basis of $â€œImâ€œ(ğ’«_Î»)$, then the collection of its non-zero components is a (classical) basis of $â€œImâ€œ(P_Î»)$ .}
\a\aa
\textbf{Proof}: Since
$$Îµ^j\vec w_i âˆˆ  â€œImâ€œ(ğ’«)$$
$$â€œConstâ€œ(Îµ^j\vec w_i) âˆˆ â€œImâ€œ(P).$$
Let $e_k$ be k'th column of $I_n$
$$ğ’« e_k = \sum_i \sum_ja_{i,j}Îµ^j\vec w_i$$
$$P e_k = \sum_i \sum_ja_{i,j}â€œConstâ€œ\left(Îµ^j\vec w_i\right)$$
Therefore, $â€œConstâ€œ\left(Îµ^j\vec w_i\right)$ spans $â€œImâ€œ(P)$.
\a\aa
Test of linearly independency. Assume $a_{i,j}$ are constant scalars with
$$ \sum_i \sum_ja_{i,j}â€œConstâ€œ\left(Îµ^j\vec w_i\right)=\vec 0 $$
$$ âŸ¹  â€œConstâ€œ\left(\underbrace{\sum_i \sum_ja_{i,j}Îµ^j\vec w_i}_{\substack{â€œinfinite eigenvectorâ€œ\\â€œeigenvalue â€œÎ»+Îµ}}\right)=\vec 0 $$

$$ âŸ¹  \sum_i \sum_ja_{i,j}Îµ^j\vec w_i=\vec 0 $$
$$ âŸ¹  a_{i,j}Îµ^j\vec w_i=\vec 0 â€œ by linearly independence of â€œ\vec w_i$$
$$ âŸ¹  a_{i,j}â€œConstâ€œ\left(Îµ^j\vec w_i\right)=\vec 0 $$
This implies non-zero vectors in set $\{â€œConstâ€œ(Îµ^j\vec w_i)\}_{i,j}$ is linearly independent.
\a\aa

Let $\vec e_j$ be $j$'th column of $I_n$. We have
$$
ğ’«_{Î»_i}\vec e_j = a_1\vec w_1+â€¦+a_l\vec w_l.
$$
$$
â€œConstâ€œ(ğ’«_{Î»_i}\vec e_j) = P_{Î»_i}\vec e_j = \underbrace{â€œConstâ€œ(a_1\vec w_1)}_{\substack{â€œlinear combination of â€œ\\â€œcomponents of â€œ\vec w_1}}+â€¦+â€œConstâ€œ(a_l\vec w_l)
$$
Therefore, components of $\{\vec w_i\}$ forms a basis of $P_{Î»_i}$

\a\aa

Since $P_{Î»_i}$ is projection, a basis of $â€œImâ€œ(P_{Î»_i})$ has $â€œtrâ€œ(P_{Î»_i})$ many element, denote these elements by
$$
v_{Î»_i,1},â€¦,v_{Î»_i,â€œtrâ€œ(P_{Î»_i})}
$$
It is basis, any vector of the form $P_{Î»_i}\vec v$ can be written into linear combinations of them.
\vfill
Collecting all these basis across all $Î»_i$, 
$$
â„°:=\{
v_{Î»_1,1},â€¦,v_{Î»_1,â€œtrâ€œ(P_{Î»_1})}
v_{Î»_i,1},â€¦,v_{Î»_i,â€œtrâ€œ(P_{Î»_i})}
â€¦
v_{Î»_k,1},â€¦,v_{Î»_k,â€œtrâ€œ(P_{Î»_k})}\}
$$
Then any vector $\vec v = P_{Î»_1}\vec v+P_{Î»_2}\vec v+â€¦+P_{Î»_k}\vec v$ can be written as linear combination of them, then $â„°$ \sws.

Counting the element, $â„°$ has $â€œtrâ€œ(P_{Î»_1})+â€œtrâ€œ(P_{Î»_2})+â€¦+â€œtrâ€œ(P_{Î»_k})=â€œtrâ€œ(I_n)=n$ many element, the dimension of the whole space. So $â„°$ is \bas. This basis is called the \x{Canonical basis}.
\a\aa
Using canonical basis , the matrix $A$ is similar to Jordan canonical form. Jordan canonical form is a \x{block-digaonal matrix} with Jordan block
$$
\m
{Î»}10\cdots0,
0{Î»}1\cdots0,
00{Î»}\ddots0,
\vdots\vdots\vdots\ddots1,
000\cdots{Î»}.
$$
\a\aa
The Following is a spctural decoMpositION oF JordAn cAnOnIcAL ForM
$$
A=\m
\1\1000000,
0\1\100000,
00\100000,
000\1\1000,
0000\1000,
00000\2\10,
000000\20,
0000000\3.
$$
characteristic polynomial $(t-1)^5(t-2)^2(t-3)$
\a\aa
$$
ğ’«_1=\m
\1{\yâˆ}{\yâˆ^2}00000,
0\1{\yâˆ}00000,
00\100000,
000\1{\yâˆ}000,
0000\1000,
00000000,
00000000,
00000000.
\;
P_1=
\m
\10000000,
0\1000000,
00\100000,
000\10000,
0000\1000,
00000000,
00000000,
00000000.
$$
$$
N_1=
\m
{\x0}\10000000,
0{\x0}\1000000,
00{\x0}000000,
000{\x0}\10000,
0000{\x0}0000,
00000000,
00000000,
00000000.
$$
\a\aa
$$
ğ’«_2=\m
00000000,
00000000,
00000000,
00000000,
00000000,
00000\1{\yâˆ}0,
000000\10,
00000000.
$$
$$
ğ’«_3=\m
00000000,
00000000,
00000000,
00000000,
00000000,
00000000,
00000000,
0000000\1.
$$
\a\aa
\exe Find the matrix $P$ such that $P^{-1}AP$ is a Jordan Canonical form, and find this Jordan canonical form.
$$
A=\m2001,{-2}225,{-2}043,10{-1}1.
$$
By calculation, the characteritic polynomial is.
$$
\det(tI_4-A)=(t-2)^3(t-3)
$$
Note , the index of $t-3$ implies that $ğ’«_3$ is a classical projection. We would first caluclate that by interpolation. 
$$
ğ’«_3=P_3=\frac{(A-2I)^3}{(3-2)^3}=\m
10{-1}{-1},
10{-1}{-1},
{-1}0{1}{1},
10{-1}{-1}.
$$
This is already a rank $1$ matrix.
\a\aa
We have no idea what $ğ’«_2$ is, let us assume
$$
ğ’«_2=P_2+N_2âˆ+N_2^2âˆ^2.
$$
Use the fact that $P_2+P_3=I_4$, we know
$$
P_2 = \m0011,{-1}111,100{-1},{-1}012.
$$
\a\aa
To find the nilpotent part, use that
$$
A = â€œConstâ€œ\left((3+Îµ)ğ’«_3+(2+Îµ)ğ’«_2\right) = 3P_3+2P_2+N_2
$$
So
$$
N_2=A-3P_3-2P_2 = \m{-1}012,{-3}036,{-1}012,0000..
$$
By a simple calculation, we see
$$
N_2^2=0
$$
Therefore
$$
ğ’«_2 = \underbrace{\m0011,{-1}111,100{-1},{-1}012.}_{P_2}+âˆ\underbrace{\m{-1}012,{-3}036,{-1}012,0000.}_{N_2}
$$
\a\aa
Or we can write
$$
ğ’«_2=\m{-âˆ}0{âˆ+1}{2âˆ+1},
{-3âˆ-1}1{3âˆ+1}{6âˆ+1},
{-âˆ+1}0{âˆ}{2âˆ-1},
{-1}012.
$$
\a\aa
Since this is not a rank $1$ matrix, we can do cross-filling decomposition
$$
\m{-âˆ}0{âˆ+1}{2âˆ+1},
{-3âˆ-1}1{3âˆ+1}{6âˆ+1},
{-âˆ+1}0{âˆ}{2âˆ-1},
{-1}012.
$$
$$
=
\m{-âˆ}0{\yâˆ+1}{2âˆ+1},
{-3âˆ+2}0{\y3âˆ+1}{6âˆ-1},
{\y-âˆ+1}\0{\eâˆ}{\y2âˆ-1},
{-1}0\12.
+
\m0\000,\3{\e1}\0{\y-2},0\000,0\000.
$$
\a\aa

Eigenvector of eigenvalue $3+Îµ$
$$
\m1,1,{-1},1.
$$
Eigenvector of eigenvalue $2+Îµ$
$$
\m0,1,0,0. â£ \m{âˆ+1},{3âˆ+1},{âˆ},1.
$$
\a\aa
All vectors can be decomposed as 
$$
a_1\m1,1,{-1},1.+a_2\m0,1,0,0.
+â€œConstâ€œ\left((a_3+a_4Îµ)\m{âˆ+1},{3âˆ+1},{âˆ},1.\right)
$$
\a\aa
However, if we just trying to decompose vectors into only classical eigenvectors, we are making $a_3=0$
$$
a_1\m1,1,{-1},1.+a_2\m0,1,0,0.
+(0+a_4Îµ)\m{âˆ+1},{3âˆ+1},{âˆ},1.
$$

$$
a_1\m1,1,{-1},1.+a_2\m0,1,0,0.
+a_4\m{1},{3},{1},0.
$$
Then there are not enough eigenvectors.
\a\aa

$$
A
\m1011,1131,{-1}010,1001.
=
\m1011,1131,{-1}010,1001.
\m3{}{}{},
{}2{}{},
{}{}2{1},
{}{}{}2.
$$
\vfill

$$
\m1011,1131,{-1}010,1001.^{-1}A\m1011,1131,{-1}010,1001.=\m3{}{}{},
{}2{}{},
{}{}2{1},
{}{}{}2.
$$
\a\aa
\exe Solve the differential equation
$$
y' = \m2001,{-2}225,{-2}043,10{-1}1.y
$$
for 
$$
y(0)=\m a,b,c,d.
$$

\sol The solution is give by
$$
y(t)=exp{\m2001,{-2}225,{-2}043,10{-1}1.t}y(0).
$$
\a\aa
By spectural decomposition
$$
g(A) = 
$$
$$g(3)\m
10{-1}{-1},
10{-1}{-1},
{-1}0{1}{1},
10{-1}{-1}.
$$
$$
+â€œConstâ€œ\left(
g(2+Îµ)
\m{-âˆ}0{âˆ+1}{2âˆ+1},
{-3âˆ-1}1{3âˆ+1}{6âˆ+1},
{-âˆ+1}0{âˆ}{2âˆ-1},
{-1}012.\right)
$$
\a\aa
Therefore
$$
e^{At} = 
$$
$$
e^{3t}\m
10{-1}{-1},
10{-1}{-1},
{-1}0{1}{1},
10{-1}{-1}.
$$
$$
+â€œConstâ€œ\left(e^{(2+Îµ)t}\m{-âˆ}0{âˆ+1}{2âˆ+1},
{-3âˆ-1}1{3âˆ+1}{6âˆ+1},
{-âˆ+1}0{âˆ}{2âˆ-1},
{-1}012.\right)
$$
Note that $e^{2t+Îµt}=e^{2t}(1+Îµt+O(Îµ^2))$
\a\aa
where
$$
â€œConstâ€œ\left(e^{(2+Îµ)t}\m{-âˆ}0{âˆ+1}{2âˆ+1},
{-3âˆ-1}1{3âˆ+1}{6âˆ+1},
{-âˆ+1}0{âˆ}{2âˆ-1},
{-1}012.\right)
$$
$$=
e^{2t}\m0011,{-1}111,100{-1},{-1}012.+te^{2t}\m{-1}012,{-3}036,{-1}012,0000.
$$
\a\aa
So the complete solution $e^{At}$ equals to
$$
e^{3t}\m
10{-1}{-1},
10{-1}{-1},
{-1}0{1}{1},
10{-1}{-1}.+e^{2t}\m0011,{-1}111,100{-1},{-1}012.+te^{2t}\m{-1}012,{-3}036,{-1}012,0000.
$$
%\a\aa


%
%$$
%\m{-âˆ}0{âˆ+1}{2âˆ+1},
%{-3âˆ+2}0{3âˆ+1}{6âˆ-1},
%{-âˆ+1}0{âˆ}{2âˆ-1},
%{-1}012.
%$$
%
%$$
%\m0000,310{-2},0000,0000.
%$$


\a\aa
%\a{A standard form of infinite cross-filling}
%
%We do a cross filling of infinite matrix. We note the cross-center 
%\a\aa
%
%\[lem]{Suppose the infinite matrix $M$ suffers an infinite cross-filling
%$$
%M = 
%v_1âˆ^{k_1}w_1^T+
%v_2âˆ^{k_2}w_2^T+
%v_mâˆ^{k_m}w_m^T
%$$
%where $v_i$ and $w_i$ are formal vectors and $âˆ^{k_i}$ is an infinite scalar. Then the list of infinite vectors
%$$ v_1âˆ^{k_1}, â£ v_2âˆ^{k_2}, â£\cdots, â£  v_mâˆ^{k_m} $$
%$$ w_1âˆ^{k_1}, â£ w_2âˆ^{k_2}, â£\cdots, â£  w_mâˆ^{k_m} $$
%are linearly independent.
%}
%If $M = 
%v_1âˆ^{k_1}w_1^T+
%v_2âˆ^{k_2}w_2^T+
%v_mâˆ^{k_m}w_m^T$ is a cross filling, so is  
%$M^T = 
%w_1âˆ^{k_1}v_1^T+
%w_2âˆ^{k_2}v_2^T+
%w_mâˆ^{k_m}v_m^T$. We only need to prove for $v_i$
%% The choice of cross-center must be biggest order, formal scalar multiple
%\a\aa
%\[defi]{A component of an infinite vector is the coefficient vector of some $âˆ^k$.
%}
%\a\aa
%\[lem]{If the constant part of an infinite eigenvector is zero vector,
%$$
%â€œConstâ€œ(\vec v)=\vec 0
%$$ then the infinite eigenvector is zero vector.
%$$
%\vec v â‰¡ \vec 0.
%$$
%}
%Letting 
%$$
%\vec v = \vec v_0 + \vec v_1âˆ + \vec v_2âˆ^2+\cdots+\vec v_kâˆ^k
%$$
%Then we have
%$$
%\vec v_k = â€œConstâ€œ(Îµ^k\vec v) = â€œConstâ€œ((A-Î»I)^k\vec v) = (A-Î»I)^kâ€œConstâ€œ(\vec v)=(A-Î»I)^k\vec 0=\vec 0.
%$$
%Therefore all components of this infinite eigenvector is zero vector, the vector is zero vector.
%\a\aa
%\[prop]{If $M$ is an infinite eigenmatrix of $A$ of eigenvalue $Î»+Îµ$ with the cross-filling of infinite projection given by 
%$M = 
%v_1âˆ^{k_1}w_1^T+
%v_2âˆ^{k_2}w_2^T+
%v_mâˆ^{k_m}w_m^T$
%Then components of 
%$$ v_1âˆ^{k_1}, â£ v_2âˆ^{k_2}, â£\cdots, â£  v_mâˆ^{k_m} $$
%and components of
%$$ w_1âˆ^{k_1}, â£ w_2âˆ^{k_2}, â£\cdots, â£  w_mâˆ^{k_m} $$
%are all linearly independent.
%}
%A linear combination of component is the constant part of a formal linear combination of these vectors, which is again infinite eigenvectors. Therefore, since the constant part of it is zero, itself is zero, all summand is zero vector, so all the coefficient is zero.
%
%\a\aa
%Recall the lemma
%$$
%P_0 = 
%\m
%||\cdots|,
%{v_1}{v_2}{\cdots}{v_k},
%||\cdots|.
%\m
%-{w_1^T}-,
%-{w_2^T}-,
%\vdots\vdots\vdots,
%-{w_k^T}-.
%$$
%is a projection, then $w_i^Tv_i=1$ and therefore $v_iw_i^T$ are rank-1 projections.  
%%% We are close to JCF, now it just how to group them, by index?
%
%
%
%%%% Do one have to introduce the infinitesimal conjugation?
%%%% After JCF, relation between infinite eigenvector, and its constant part. 
%
%
%%%% Just by the part of JCF, describe what is in P, and so 
%
%
%%%% Describe the relation between Spectural decomposition and JCF
%
%%%% Describe the spectural theorem, like specutral mapping theorem. 
%
%%%% Describe the rank and the equation.
%
%%%%%  How to understand the minimal polynomial criterion. 
%
%
%%%% How to visualize the Calay Hamilton Theorem from Jordan Canonical Form.
%
%
%%%% A question o f what matrix is diagonalizable.
%
%%%% Normal matrices are diagonalizable.
%
%%%% A^HA = 0 then A = 0
%
%%% A generalization of whatever whatever
%%%% P is commute with P^H, then (P-PP^H)(P^H-PP^H)=0  so P=PP^H=P^H, then P^H is hermitian orthogonal projection. and thus in fact interesting.
%
%
%
%
%
%
%
\aaa


\aaa{Data Fitting}
As an application, we introduce the orthogonal projection to data fitting problem.
\a\aa
\exe Suppose one has the following data. 
~\\
\begin{tabular}{|c|c|c|c|c|}
	\hline
	 &X=0.3&X=0.9&X=1.1&X=1.7   \\
	 \hline
	 Y&2&1&-1&-2\\
	\hline
\end{tabular}
    

You want to find a function $D(x)=ax+b$ that fit these data in a way such that the error:
$$
E(D):=||D(0.3)-2||² +
||D(0.9)-1||² +
||D(1.1)-(-1)||²+ 
||D(1.7)-(-2)||² 
$$
is minimal for your choice of function $D$.

\begin{tikzpicture}[scale=0.25]
    \wangge{-7}{-7}77
    \draw[->,thick] (-8,0)--(8,0) node[below] {x};
    \draw[->,thick] (0,-8)--(0,8) node[right] {y};
    \draw[fill,red] (0.3,2) circle[radius=0.2];
    \draw[fill,red] (0.9,1) circle[radius=0.2];
    \draw[fill,red] (1.1,-1) circle[radius=0.2];
    \draw[fill,red] (1.7,-2) circle[radius=0.2];
\end{tikzpicture}

\a\aa
Model: 
$$D(x) = ax+b$$

Regression Error:
$$
E(D):=||D(0.3)-2||² +
||D(0.9)-1||² +
||D(1.1)-(-1)||²+ 
||D(1.7)-(-2)||² 
$$


Idea: We translate the problem into a geometric problem 
\[itemize]{
\item Data ⟺   A point, 

\item Model ⟺   point on a subspace;   

\item Error ⟺   Distance.
}
\a{Data as a point in a space}
We may think the data 

\t
{$X=0.3$}
{$X=0.9$}
{$X=1.1$}
{$X=1.7$},
21{-1}{-2}.

as a given vector in $ℝ^4$. This space is representing all possible observations.
$$
\m
2,1,{-1},{-2}.
$$

\a{Model as subspace}
The model is always selected as what the perfect data should be. In our model $D(x)=ax+b$. In terms of $a$ and $b$, the perfect data can be listed as follows

\t
{$X=0.3$}
{$X=0.9$}
{$X=1.1$}
{$X=1.7$},
{$0.3a+b$}
{$0.9a+b$}
{$1.1a+b$}
{$1.7a+b$}.


For each $a$ and $b$, it associates to a point, but the collection of all such points gives a subset
$$
\left\{
\m
{0.3a+b},
{0.9a+b},
{1.1a+b},
{1.7a+b}.
: “ for “a,b ∈ ℝ 
\right\}
= 
\left\{
\m
{0.3}1,
{0.9}{1},
{1.1}{1},
{1.7}{1}.
\m a,b.
: “ for “\m a,b. ∈ ℝ ² 
\right\}
$$
$$
=“ Col “\m
{0.3}1,
{0.9}{1},
{1.1}{1},
{1.7}{1}.
$$
\a\aa
With this understanding, the Error $$
E(D):=||D(0.3)-2||² +
||D(0.9)-1||² +
||D(1.1)-(-1)||²+ 
||D(1.7)-(-2)||² 
$$
is the distance of 
$$
\m
2,1,{-1},{-2}.
$$
to a given point on the 
$$
“ Col “\m
{0.3}1,
{0.9}{1},
{1.1}{1},
{1.7}{1}.
$$
Our first step is to formulate orthogonal projections to the perfect data space
\a\aa
$$
\m
{0.3}1,
{0.9}{1},
{1.1}{1},
{1.7}{1}.
\left(
\m
{0.3}
{0.9}
{1.1}
{1.7},
1111.
\m
{0.3}1,
{0.9}{1},
{1.1}{1},
{1.7}{1}.
\right)^{-1}
\m
{0.3}
{0.9}
{1.1}
{1.7},
1111.
$$
$$
=\m
{0.3}1,
{0.9}{1},
{1.1}{1},
{1.7}{1}.
\m54,44.^{-1}
\m
{0.3}
{0.9}
{1.1}
{1.7},
1111.
$$
$$
=
\underbrace{\m
{0.3}1,
{0.9}{1},
{1.1}{1},
{1.7}{1}.}_{“keep left factor“}
\m{-0.7}{-0.1}{0.1}{0.7},
{0.95}{0.35}{0.15}{-0.45}.
$$
\a\aa
Therefore, the best data is given by
$$
\m
{0.3}1,
{0.9}{1},
{1.1}{1},
{1.7}{1}.
\m{-0.7}{-0.1}{0.1}{0.7},
{0.95}{0.35}{0.15}{-0.45}.\m2,1,{-1},{-2}.
=
\m
{0.3}1,
{0.9}{1},
{1.1}{1},
{1.7}{1}.
\m{-3},{3}.
$$
Note that this is of the form
$$
\m
{0.3}1,
{0.9}{1},
{1.1}{1},
{1.7}{1}.
\m{a},{b}.
$$
for $a=3$, $b=3$. 
Therefore, the best regression is 
$$
D(x) = -3x+3.
$$
%$$
%\xequal{\substack{r_1:=r_1-r_2\\c_1:=c_1-c_2}}\m
%{-0.7}1,
%{-0.1}{1},
%{0.1}{1},
%{0.7}{1}.
%\m10,04.^{-1}
%\m
%{-0.7}
%{-0.1}
%{0.1}
%{0.7},
%1111.
%$$
%The projection we found is
%$$
%\m{0.74}{0.32}{0.18}{-0.24},
%{0.32}{0.26}{0.24}{0.18},
%{0.18}{0.24}{0.26}{0.32},
%{-0.24}{0.18}{0.32}{0.74}.
%$$
\a\aa
\[columns]{\co6
\begin{tikzpicture}[scale=0.35]
    \wangge{-7}{-7}77
    \draw[->,thick] (-8,0)--(8,0) node[below] {x};
    \draw[->,thick] (0,-8)--(0,8) node[right] {y};
    \draw[fill,red] (0.3,2) circle[radius=0.2];
    \draw[fill,red] (0.9,1) circle[radius=0.2];
    \draw[fill,red] (1.1,-1) circle[radius=0.2];
    \draw[fill,red] (1.7,-2) circle[radius=0.2];
    \draw[blue] (4,-9)--(-2,9);
\end{tikzpicture}
\co 4
The best fitting function for the printed data.
}
\a\aa
\exe Find the best parabola fit $b(t)=C+Dt+Et^2$ to the data points
$$
(t_1,b_1)=(-1,5), ␣ 
(t_2b_2)=(0,2), ␣ 
(t_3b_3)=(1,1), ␣ 
(t_4b_4)=(2,2).
$$ 
in the sense of minimizing 
$$
||E||²=(b_1-b(t_1))^2 + 
(b_2-b(t_2))^2+
(b_3-b(t_3))^2+
(b_4-b(t_4))^2
$$
\a\aa
\sol Consider the point of obeserved data
$$
\m
{b_1},
{b_2},
{b_3},
{b_4}.
=\m5,2,1,2. ∈ ℝ^4
$$
The map $b(t) ↦ \m
{b(-1)},
{b(0)},
{b(1)},
{b(2)}.
$
maps perfect data into a subspace into $ℝ^4$. The error function 
$$
||E||²=(b_1-b(t_1))^2 + 
(b_2-b(t_2))^2+
(b_3-b(t_3))^2+
(b_4-b(t_4))^2
$$ 
describe the distance of an observed data to the subspace of perfect data. \x{Therefore, we may find perfect data by the orthogonal projection formula.}
\a\aa
For this, we need to describe the subspace of perfect data, which is all possible 
$$
\vec v\m{b(-1)},
{b(0)},
{b(1)},
{b(2)}.
=
\m{C-D+E},{C},{C+D+E},{C+2D+4E}.= \m1{-1}1,100,111,124.\m C,D,E.
$$
for any $b(t) = C+Dt+Et^2$.

Our goal is to find the orthogonal projection $P_W$ to the perfect data space 
$$
W = “Col“\m1{-1}1,100,111,124.
$$
\a\aa
Of course, you may find it by the orthogonal projection formula
$$
P_W = \m1{-1}1,100,111,124.\left(\m1111,{-1}012,1014.\m1{-1}1,100,111,124.\right)^{-1}\m1111,{-1}012,1014.
$$
And the perfect data closest to the observed data is given by
$$
P_W\vec v = {\tiny\m1{-1}1,100,111,124.\underbrace{\left(\m1111,{-1}012,1014.\m1{-1}1,100,111,124.\right)^{-1}\m1111,{-1}012,1014. \m5,2,1,2. }_{“Calculate this part you will get “(C,D,E)^T}}
$$
We omit the rest steps here.
\a\aa
However, there is a \x{Cool} method for this problem.

\a\aa
\x{First trick, from orthogonal complement}.
$$
W = “Col“\m1{-1}1,100,111,124.
$$
is 3-dimensional. Hard. But $W^\perp$ is 1-dimensional! Good!

\[itemize]{
\item Instead of finding $P_W$, find $P_{W^\perp}$ and so $P_W=I-P_{W^\perp}$
}
\a\aa
\x{Second trick, By lagurange interpolation polynomial}
$$
P_W(\vec v)  = \m
{b_{\text{answer}}(-1)},
{b_{\text{answer}}(0)},
{b_{\text{answer}}(1)},
{b_{\text{answer}}(2)}.
$$
do not directly give us the $C,D,E$, but it we know the value of $b(t)$ at $t=-1,0,1,2$. We may use \x{Lagrange Interpolation Polynomial} 
$$
b(t) = 
b(-1)\frac{x(x-1)}{(-1-0)(-1-1)}
+
b(0)\frac{(x+1)(x-1)}{(0+1)(0-1)}
+
b(1)\frac{(x+1)x}{(1+1)(1-0)}
$$
\a\aa
\sol Let us do. Firstly , by solving the equation, we find 
$$
W^\perp = “Null“\m1111,{-1}012,1014.=“Col“\m1,{-3},3,{-1}.
$$
Then
$$
P_{W^\perp} = \frac{\m1,{-3},3,{-1}.\m1{-3}3{-1}.}{\m1{-3}3{-1}.\m1,{-3},3,{-1}.} = \frac1{20}\m1{-3}3{-1},{-3}9{-9}3,3{-9}9{-3},{-1}3{-3}{1}.
$$
\a\aa
Therefore,
$$
P_W = \frac1{20}\m{19}3{-3}1,3{11}9{-3},{-3}9{11}3,1{-3}3{19}.
$$
So
$$
\m
{b_{\text{answer}}(-1)},
{b_{\text{answer}}(0)},
{b_{\text{answer}}(1)},
{b_{\text{answer}}(2)}.
=P_W\m5,2,1,2.=\m{5},{2},{1},{2}.
$$
Just by looking at the data, we observe 
$$
b_{\text{answer} }= (x-1)² + 1
$$


\aaa
